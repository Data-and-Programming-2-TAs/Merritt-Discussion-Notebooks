{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Plotting w/ Bokeh\n",
    "\n",
    "Today we'll be going over how to do some more advanced things with interactive plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "from config import gmaps_key\n",
    "from pathlib import Path\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, gmap\n",
    "from bokeh import events\n",
    "from bokeh.models import CustomJS, Div, Button, GMapOptions, Dropdown, ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import column, row\n",
    "output_notebook()\n",
    "\n",
    "data_path = Path.cwd() / 'data/boston_crime.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Encodings\n",
    "Depending on the application that was used to create a CSV and the location it was created in, we could see different encodings on a CSV. For example, older Excel versions used custom encodings like what Pandas calls 'cp1252', and other countries have different encodings based on their local language. This has largely gone by the wayside in modern work since everything should be UTF-8, but you will see non-standard encodings every so often. \n",
    "\n",
    "[Here's the encodings read_csv supports.](https://docs.python.org/3/library/codecs.html#standard-encodings)\n",
    "\n",
    "The best way of telling what type of encoding you're dealing with is to make sure that the person giving you the data puts it in UTF-8. That's not always possible, but for those times there's chardet. Chardet is a Python library that uses machine learning to predict what kind of encoding the file has. It takes a long time for large files and isn't always accurate, but it's usually a good thing to try. \n",
    "\n",
    "I don't recommend that you run the code chunk below. It takes forever, because this dataset is enormous. The output does state that the encoding is 'latin-1', which is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(data_path):\n",
    "    \"\"\"\n",
    "    detect_encoding()\n",
    "    Takes in a Path object and prints the predicted encoding and confidence.\n",
    "    \n",
    "    Gets: data_path, a Path object\n",
    "    Returns: nothing\n",
    "    \"\"\"\n",
    "    with open(data_path, 'rb') as read_file:\n",
    "        print(chardet.detect(read_file.read()))\n",
    "        \n",
    "        \n",
    "detect_encoding(data_path)\n",
    "# results in 'latin-1' but takes forever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "The best way to write comments in Python is, in my opinion and in the opinion of PEP8, to use docstrings. The comments I've placed within these functions are examples of docstrings. \n",
    "\n",
    "For more on docstrings, [see here](https://www.python.org/dev/peps/pep-0257/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(data_path):\n",
    "    \"\"\"\n",
    "    import_data(data_path)\n",
    "    Receives a Path object and uses that to read in a csv\n",
    "    and return it. Currently hardcoding encoding because this\n",
    "    will only be used for one csv.\n",
    "    \n",
    "    Gets: data_path, a Path object\n",
    "    Retuns: a Pandas Dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_csv(data_path, encoding='latin-1')\n",
    "    \n",
    "    \n",
    "df = import_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Lat'].notnull()) & (df['Long'].notnull())]\n",
    "df = df[(df['Lat'] > 41) & (df['Lat'] < 43)]\n",
    "df = df[(df['Long'] > -73) & (df['Long'] < -69)]\n",
    "df = df.sample(frac=.05, axis = 'index')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(df)\n",
    "\n",
    "map_options = GMapOptions(lat=42.359955, lng=-71.059886, map_type=\"roadmap\", zoom=11)\n",
    "tooltips = [\n",
    "    (\"Date\", \"@OCCURRED_ON_DATE\"),\n",
    "    (\"Offense Description\", \"@OFFENSE_DESCRIPTION\"),\n",
    "]\n",
    "\n",
    "\n",
    "p = gmap(gmaps_key, title=\"Boston Crime\", map_options=map_options, tools=\"box_select\")\n",
    "p.circle('Long', 'Lat', size=2, fill_alpha=0.6, line_color=None, source=source)\n",
    "div = Div(width=400)\n",
    "layout = column(button, row(p, div))\n",
    "p.add_tools(HoverTool(tooltips=tooltips))\n",
    "\n",
    "p.js_on_event(events.SelectionGeometry, CustomJS(args=dict(div=div), code=\"\"\"\n",
    "div.text = \"Selection! <p> <p>\" + JSON.stringify(cb_obj.geometry, undefined, 2);\n",
    "\"\"\"))\n",
    "\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
