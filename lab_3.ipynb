{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Spacy\n",
    "\n",
    "I suggest that you read the documentation for yourself. The best way to learn how to do something in Python is uasually to read the documentation and Google your question. \n",
    "\n",
    "[Here is the spacy documentation](https://spacy.io/usage/spacy-101).\n",
    "\n",
    "The data we'll be working with today is a set of all the Jeopardy questions and their matched answers, along with some information about the category, value, etc. We're mostly interested in the questions, and that's what we'll use spacy for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpdy_path = Path.cwd() / 'data' / 'JEOPARDY_CSV.csv'\n",
    "jpdy = pd.read_csv(jpdy_path)\n",
    "jpdy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
      "       ' Question', ' Answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(jpdy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpdy.columns = [c.strip() for c in jpdy.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216930\n"
     ]
    }
   ],
   "source": [
    "print(len(jpdy.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to cut jpdy to a tenth of its size, because my laptop doesn't have enough memory or processing power to run nlp() on every single question in this database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpdy = jpdy.iloc[:int(len(jpdy.index)/10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21693\n"
     ]
    }
   ],
   "source": [
    "print(len(jpdy.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 148.8704149723053 seconds\n",
      "0    (For, the, last, 8, years, of, his, life, ,, G...\n",
      "1    (No, ., 2, :, 1912, Olympian, ;, football, sta...\n",
      "2    (The, city, of, Yuma, in, this, state, has, a,...\n",
      "3    (In, 1963, ,, live, on, \", The, Art, Linklette...\n",
      "4    (Signer, of, the, Dec., of, Indep, ., ,, frame...\n",
      "Name: Question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "jpdy['Question'] = [nlp(q) for q in jpdy['Question']]\n",
    "end = time.time()\n",
    "print(f'Elapsed time: {end - start} seconds')\n",
    "print(jpdy['Question'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the elapsed time above, even after I set it to one tenth of all my data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of Spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, years, life, ,, Galileo, house, arrest, espousing, man, theory] \n",
      "\n",
      "8 NUM CD d False False False\n",
      "years NOUN NNS xxxx True False False\n",
      "life NOUN NN xxxx True False False\n",
      ", PUNCT , , False True False\n",
      "Galileo PROPN NNP Xxxxx True False False\n",
      "house NOUN NN xxxx True False False\n",
      "arrest NOUN NN xxxx True False False\n",
      "espousing VERB VBG xxxx True False False\n",
      "man NOUN NN xxx True False False\n",
      "theory NOUN NN xxxx True False False\n"
     ]
    }
   ],
   "source": [
    "first_question = jpdy.iloc[0]['Question']\n",
    "print(first_question, '\\n')\n",
    "\n",
    "for word in first_question:\n",
    "    print(word.text, word.pos_, word.tag_, word.shape_, word.is_alpha, word.is_punct, word.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "A common task in NLP is trying to measure how similar two documents are semantically. Spacy has a way for us to do that. Let's try to figure out the most similar question to our first question. To do that, let's first take out the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 years life , Galileo house arrest espousing man theory\n",
      "<class 'type'>\n"
     ]
    }
   ],
   "source": [
    "new_question_list = []\n",
    "for question_index, question in enumerate(jpdy['Question']):\n",
    "    new_question_list.append(nlp(' '.join([token.text for token in question if not token.is_stop])))\n",
    "        \n",
    "jpdy['Question'] = new_question_list\n",
    "first_question = jpdy.iloc[0]['Question']\n",
    "print(jpdy.iloc[0]['Question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-44a606721d81>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  jpdy['Similarity to Question 1'] = [q.similarity(first_question) for q in jpdy['Question']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9358     (1287, storm, flooded, land, separating, North...\n",
      "2536     (chapter, 52, novel, ,, boisterous, crowd, gat...\n",
      "1541     (Da, ,, comrade--, fork, pierces, bird, ,, lau...\n",
      "11850    (1903, Jack, London, work, dog, ,, half, St., ...\n",
      "6841     ((, <, href=\"http://www.j, -, archive.com, /, ...\n",
      "Name: Question, dtype: object\n",
      "1287 storm flooded land separating North Sea & Zuiderzee , turning village major port city\n",
      "chapter 52 novel , boisterous crowd gathering Fagin execution\n",
      "Da , comrade-- fork pierces bird , launching jet fragrant melted butter chicken\n",
      "1903 Jack London work dog , half St. Bernard , half Scotch shepherd , survives wilderness\n",
      "( < href=\"http://www.j - archive.com / media/2007 - 12 - 26_DJ_27.jpg \" target=\"_blank\">A trainer dog stop curb Cheryl Clue Crew Seeing Eye New Jersey.</a > )    dog trained stop curbs reasons , safety & orientation , people visually impaired determine location counting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-44a606721d81>:1: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  jpdy['Similarity to Question 1'] = [q.similarity(first_question) for q in jpdy['Question']]\n"
     ]
    }
   ],
   "source": [
    "jpdy['Similarity to Question 1'] = [q.similarity(first_question) for q in jpdy['Question']]\n",
    "jpdy = jpdy.sort_values(by = 'Similarity to Question 1', ascending = False)\n",
    "print(jpdy['Question'][1:6])\n",
    "for question in jpdy['Question'][1:6]:\n",
    "    print(' '.join([doc.text for doc in question]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jpdy.tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy in practice\n",
    "I used spacy to turn text into features (or variables) for a machine learning pipeline that I created at my job. First, to be in comliance with the GDPR, I had to take out all potentially personally identifying information in the text. I was able to do that with Spacy, because I can set up rules like \"if word.pos_ == 'PROPN': sentence[index] = word.shape. I was able to take out stop words, punctuation and proper nouns, conduct sentiment analysis, and create more variables to use in my anomaly detection model because I used spacy to make the unstructred data that I had into a more structured format. \n",
    "\n",
    "The other thing about Spacy is that it takes a LONG TIME to do things. Neural networks take a lot of computation, and they're generally better suited for GPUs than CPUs, but laptops have much more in the way of CPUs than GPUs if they have one at all. Usually you'll run Spacy on a server if you're trying to apply it on a large dataset like this. In my previous work I had a 16 core server and it still took Spacy quite a while to work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
